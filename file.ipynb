{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/betamaan/Demo3/blob/main/file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### requirements"
      ],
      "metadata": {
        "id": "zHMuSWqo_REO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langchain\n",
        "langchain-core\n",
        "\n",
        "langchain-openai\n",
        "openai\n",
        "\n",
        "langchain-anthropic\n",
        "\n",
        "langchain-google-genai\n",
        "google-generativeai\n",
        "\n",
        "langchain-huggingface\n",
        "transformers\n",
        "huggingface-hub\n",
        "\n",
        "python-dotenv\n",
        "\n",
        "numpy\n",
        "scikit-learn\n"
      ],
      "metadata": {
        "id": "04IxYvrkdmvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Agent 1\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "from dotenv import load_dotenv\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "loader = PyPDFLoader(\"/home/skumar/Langchain/file/paper81.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "citation_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Extract ONLY citation numbers from the academic text.\n",
        "\n",
        "Citations look like [1], [2], (3), etc.\n",
        "\n",
        "Return a plain JSON list of integers like:\n",
        "[1, 2, 3]\n",
        "\n",
        "Do not return extra text or markdown.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = citation_prompt | model | parser\n",
        "\n",
        "all_citation_numbers = set()\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk.page_content})\n",
        "        try:\n",
        "            numbers = json.loads(result)\n",
        "        except json.JSONDecodeError:\n",
        "            numbers = re.findall(r\"\\b\\d{1,4}\\b\", result)\n",
        "            numbers = list(map(int, numbers))\n",
        "        all_citation_numbers.update(numbers)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "filtered_citation_numbers = sorted([\n",
        "    int(n) for n in all_citation_numbers\n",
        "    if str(n).isdigit() and 1 <= int(n) <= 300\n",
        "])\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(\"results/citation_numbers.json\", \"w\") as f:\n",
        "    json.dump(filtered_citation_numbers, f)\n",
        "\n",
        "with open(\"results/chunks.json\", \"w\") as f:\n",
        "    json.dump([{\"page_content\": chunk.page_content} for chunk in chunks], f)\n",
        "\n",
        "print(\"Total citation numbers found:\", len(filtered_citation_numbers))\n",
        "print(\"Saved to results/citation_numbers.json\")\n"
      ],
      "metadata": {
        "id": "doYUxnc-iNFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Step 0: Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Step 1: Load PDF\n",
        "loader = PyPDFLoader(\"/home/skumar/Langchain/file/paper66.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# Step 2: Split into manageable chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# Step 3: Setup LLM model and prompt\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "citation_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Extract all citation references from the academic text.\n",
        "\n",
        "Citations can look like:\n",
        "- [1], [2]\n",
        "- (Smith et al., 2021)\n",
        "- [Touvron et al., 2023]\n",
        "- (Research, 2022)\n",
        "\n",
        "Return a plain JSON list of citation strings that exactly match what‚Äôs in the text, like:\n",
        "[\n",
        "  \"[Touvron et al., 2023]\",\n",
        "  \"(Research, 2022)\",\n",
        "  \"[1]\",\n",
        "  \"[2]\"\n",
        "]\n",
        "\n",
        "Do not add extra commentary or markdown.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = citation_prompt | model | parser\n",
        "\n",
        "all_citation_strings = set()\n",
        "\n",
        "print(\"üîç Extracting citations from chunks...\")\n",
        "for i, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk.page_content})\n",
        "        citations = json.loads(result)\n",
        "        citations = [c.strip() for c in citations if isinstance(c, str) and len(c.strip()) > 2]\n",
        "        all_citation_strings.update(citations)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i}: {e}\")\n",
        "\n",
        "print(\"üîç Verifying extracted citations exist in text...\")\n",
        "verified_citations = set()\n",
        "\n",
        "all_text = \" \".join(chunk.page_content for chunk in chunks)\n",
        "\n",
        "for citation in sorted(all_citation_strings):\n",
        "    if citation in all_text:\n",
        "        verified_citations.add(citation)\n",
        "\n",
        "sorted_citations = sorted(verified_citations)\n",
        "citation_number_map = {i + 1: cite for i, cite in enumerate(sorted_citations)}\n",
        "citation_numbers = list(citation_number_map.keys())\n",
        "\n",
        "\n",
        "# Step 6: Save to files\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "with open(\"results/citation_numbers.json\", \"w\") as f:\n",
        "    json.dump(citation_numbers, f, indent=2)\n",
        "\n",
        "with open(\"results/citation_map.json\", \"w\") as f:\n",
        "    json.dump(citation_number_map, f, indent=2)\n",
        "\n",
        "with open(\"results/chunks.json\", \"w\") as f:\n",
        "    json.dump([{\"page_content\": chunk.page_content} for chunk in chunks], f, indent=2)\n",
        "\n",
        "# Step 7: Final summary\n",
        "print(\"Citation extraction complete.\")\n",
        "print(f\"Total unique citations found: {len(citation_number_map)}\")\n",
        "print(f\"Results saved to: results/citation_numbers.json and citation_map.json\")\n",
        "print(\"Sample citation map:\", list(citation_number_map.items())[:5])\n"
      ],
      "metadata": {
        "id": "zSBCllU_iRdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "from dotenv import load_dotenv\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "loader = PyPDFLoader(\"/home/skumar/Langchain/file/paper81.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "citation_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Extract ONLY citation numbers from the academic text.\n",
        "\n",
        "Citations look like [1], [2], (3), etc.\n",
        "\n",
        "Return a plain JSON list of integers like:\n",
        "[1, 2, 3]\n",
        "\n",
        "Do not return extra text or markdown.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = citation_prompt | model | parser\n",
        "\n",
        "all_citation_numbers = set()\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk.page_content})\n",
        "        try:\n",
        "            numbers = json.loads(result)\n",
        "        except json.JSONDecodeError:\n",
        "            numbers = re.findall(r\"\\b\\d{1,4}\\b\", result)\n",
        "            numbers = list(map(int, numbers))\n",
        "        all_citation_numbers.update(numbers)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "candidate_citations = sorted([\n",
        "    int(n) for n in all_citation_numbers\n",
        "    if str(n).isdigit() and 1 <= int(n) <= 300\n",
        "])\n",
        "\n",
        "verified_citations = set()\n",
        "\n",
        "for n in candidate_citations:\n",
        "    pattern_square = rf\"\\[{n}\\]\"\n",
        "    pattern_round = rf\"\\({n}\\)\"\n",
        "    found = False\n",
        "    for chunk in chunks:\n",
        "        if re.search(pattern_square, chunk.page_content) or re.search(pattern_round, chunk.page_content):\n",
        "            verified_citations.add(n)\n",
        "            break\n",
        "\n",
        "filtered_citation_numbers = sorted(verified_citations)\n",
        "\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(\"results/citation_numbers.json\", \"w\") as f:\n",
        "    json.dump(filtered_citation_numbers, f)\n",
        "\n",
        "with open(\"results/chunks.json\", \"w\") as f:\n",
        "    json.dump([{\"page_content\": chunk.page_content} for chunk in chunks], f)\n",
        "\n",
        "print(\"Total citation numbers found:\", len(filtered_citation_numbers))\n",
        "print(\"Saved to results/citation_numbers.json\")\n"
      ],
      "metadata": {
        "id": "nmHCP3rqiXSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Agent2\n",
        "\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Load model\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Load previous results\n",
        "with open(\"results/citation_numbers.json\") as f:\n",
        "    citation_numbers = json.load(f)\n",
        "\n",
        "with open(\"results/chunks.json\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "# Prompt to extract author + year\n",
        "author_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are given a section of an academic paper. Extract a list of properly formatted citation entries.\n",
        "\n",
        "Each entry must have:\n",
        "- The citation number\n",
        "- The full author list\n",
        "\n",
        "Format each citation like this:\n",
        "Citation No. 1: Kumar R., Sharma V.\n",
        "\n",
        "Return as JSON list:\n",
        "[\n",
        "  {{\"citation_no\": 1, \"author\": \"Kumar R., Sharma V.\"}},\n",
        "  ...\n",
        "]\n",
        "\n",
        "Only include entries with author . Do not anything else except this.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = author_prompt | model | parser\n",
        "\n",
        "final_citations = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1}\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk[\"page_content\"]})\n",
        "        match = re.search(r\"\\[.*\\]\", result.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "        data = json.loads(match.group())\n",
        "\n",
        "        for entry in data:\n",
        "            cnum = entry.get(\"citation_no\")\n",
        "            author = entry.get(\"author\")\n",
        "            if (\n",
        "                cnum in citation_numbers and\n",
        "                author and\n",
        "                \"not available\" not in author.lower() and\n",
        "                \"n/a\" not in author.lower()\n",
        "            ):\n",
        "                final_citations[int(cnum)] = f\"{author}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i+1}: {e}\")\n",
        "\n",
        "# Save final clean result\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(\"results/clean_citations.json\", \"w\") as f:\n",
        "    json.dump(final_citations, f, indent=2)\n",
        "\n",
        "print(\"\\n Clean Citations Extracted:\")\n",
        "for cnum in sorted(final_citations):\n",
        "    print(f\"Citation {cnum}: {final_citations[cnum]}\")\n"
      ],
      "metadata": {
        "id": "tWmUc9ediamU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Agent3\n",
        "\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "with open(\"results/citation_numbers.json\") as f:\n",
        "    citation_numbers = json.load(f)\n",
        "\n",
        "with open(\"results/chunks.json\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "with open(\"results/authors.json\") as f:\n",
        "    author_map = json.load(f)\n",
        "\n",
        "year_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are given a chunk of an academic research paper.\n",
        "\n",
        "Your task is to find the publication year for each in-text citation.\n",
        "Only return years if they are clearly associated with a citation.\n",
        "\n",
        "Return the result in this JSON format:\n",
        "[\n",
        "  {{\"citation_no\": 1, \"year\": \"2023\"}},\n",
        "  {{\"citation_no\": 2, \"year\": \"2020\"}}\n",
        "]\n",
        "\n",
        "Do not include citations without a year. Skip missing or unclear entries.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = year_prompt | model | parser\n",
        "\n",
        "citation_years = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1}\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk[\"page_content\"]})\n",
        "        match = re.search(r\"\\[.*\\]\", result.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "        data = json.loads(match.group())\n",
        "\n",
        "        for entry in data:\n",
        "            cnum = entry.get(\"citation_no\")\n",
        "            year = entry.get(\"year\")\n",
        "            if (\n",
        "                cnum in citation_numbers and\n",
        "                year and\n",
        "                re.match(r\"^(19|20)\\d{2}$\", str(year))\n",
        "            ):\n",
        "                citation_years[int(cnum)] = year\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i+1}: {e}\")\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(\"results/years.json\", \"w\") as f:\n",
        "    json.dump(citation_years, f, indent=2)\n",
        "\n",
        "print(\"\\n Verified Citation Years:\")\n",
        "for cnum in sorted(citation_years):\n",
        "    print(f\"Citation {cnum}: {citation_years[cnum]}\")\n"
      ],
      "metadata": {
        "id": "3yTT4YfTijOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Agent4"
      ],
      "metadata": {
        "id": "SUP1izqCisNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "with open(\"results/citation_numbers.json\") as f:\n",
        "    citation_numbers = json.load(f)\n",
        "\n",
        "with open(\"results/chunks.json\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "with open(\"results/authors.json\") as f:\n",
        "    author_map = json.load(f)\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Given this chunk of a research paper, generate a unique 2-line summary for each citation.\n",
        "\n",
        "Format as a JSON list:\n",
        "[\n",
        "  {{\"citation_no\": 1, \"summary\": \"This study explored ...\"}},\n",
        "  {{\"citation_no\": 2, \"summary\": \"Authors evaluated ...\"}}\n",
        "]\n",
        "\n",
        "Do not repeat the same summary.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = summary_prompt | model | parser\n",
        "\n",
        "citation_summaries = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1}\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk[\"page_content\"]})\n",
        "        match = re.search(r\"\\[.*\\]\", result.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "        data = json.loads(match.group())\n",
        "\n",
        "        for entry in data:\n",
        "            cnum = entry.get(\"citation_no\")\n",
        "            summary = entry.get(\"summary\")\n",
        "            if (\n",
        "                cnum in citation_numbers and\n",
        "                summary and\n",
        "                \"not available\" not in summary.lower() and\n",
        "                \"n/a\" not in summary.lower() and\n",
        "                cnum not in citation_summaries\n",
        "            ):\n",
        "                citation_summaries[int(cnum)] = summary.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i+1}: {e}\")\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(\"results/summaries.json\", \"w\") as f:\n",
        "    json.dump(citation_summaries, f, indent=2)\n",
        "\n",
        "print(\"\\n Citation Summaries Extracted:\")\n",
        "for cnum in sorted(citation_summaries):\n",
        "    print(f\"Citation {cnum}: {citation_summaries[cnum]}\")\n"
      ],
      "metadata": {
        "id": "zQ8s6C7Pir_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Verifier\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "with open(\"results/citation_numbers.json\") as f:\n",
        "    citation_numbers = json.load(f)\n",
        "\n",
        "with open(\"results/authors.json\") as f:\n",
        "    authors = json.load(f)\n",
        "\n",
        "with open(\"results/years.json\") as f:\n",
        "    years = json.load(f)\n",
        "\n",
        "with open(\"results/summaries.json\") as f:\n",
        "    summaries = json.load(f)\n",
        "\n",
        "verified_citations = {}\n",
        "\n",
        "for cnum in citation_numbers:\n",
        "    cnum = int(cnum)\n",
        "    if (\n",
        "        str(cnum) in authors and\n",
        "        str(cnum) in years and\n",
        "        str(cnum) in summaries\n",
        "    ):\n",
        "        verified_citations[cnum] = {\n",
        "            \"author\": authors[str(cnum)],\n",
        "            \"year\": years[str(cnum)],\n",
        "            \"summary\": summaries[str(cnum)]\n",
        "        }\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(\"results/verified_citations.json\", \"w\") as f:\n",
        "    json.dump(verified_citations, f, indent=2)\n",
        "\n",
        "print(\"\\n Verified Citations (All Conditions Met):\")\n",
        "for cnum in sorted(verified_citations):\n",
        "    entry = verified_citations[cnum]\n",
        "    print(f\"Citation {cnum}: {entry['author']} ({entry['year']}) - {entry['summary']}\")\n"
      ],
      "metadata": {
        "id": "fEqBzVyfivw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "from tabulate import tabulate\n",
        "import os\n",
        "\n",
        "# Load the verified citation data\n",
        "with open(\"results/verified_citations.json\") as f:\n",
        "    verified = json.load(f)\n",
        "\n",
        "# Prepare tabular data\n",
        "table = []\n",
        "for cnum in sorted(verified):\n",
        "    entry = verified[cnum]\n",
        "    table.append([\n",
        "        cnum,\n",
        "        entry[\"author\"],\n",
        "        entry[\"year\"],\n",
        "        entry[\"summary\"]\n",
        "    ])\n",
        "\n",
        "# Print table to console\n",
        "headers = [\"Citation No.\", \"Author(s)\", \"Year\", \"Summary\"]\n",
        "print(\"\\n Verified Citation Table:\\n\")\n",
        "print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "# Save as CSV\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "csv_file = \"results/verified_citations.csv\"\n",
        "with open(csv_file, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(headers)\n",
        "    writer.writerows(table)\n",
        "\n",
        "print(f\"\\n Saved to {csv_file}\")\n"
      ],
      "metadata": {
        "id": "w_s8q0cji8-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Agent 1\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import re\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "with open(\"/home/skumar/Langchain/.venv/results/summaries.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "items = [(k, v) for k, v in data.items()]\n",
        "string_list = [f\"[{k}] {v}\" for k, v in items]\n",
        "\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "cluster_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are a helpful assistant. Group the following strings into **semantically meaningful clusters**.\n",
        "\n",
        "Each string is prefixed with its citation number like [1], [2], etc.\n",
        "\n",
        "Clusters may be flat (simple lists of citation numbers), or hierarchical (with subcategories like \"part a\", \"subgroup 1.1\", etc.).\n",
        "\n",
        "Your output must be a valid JSON object. Examples of valid formats:\n",
        "\n",
        "Flat:\n",
        "{{\n",
        "  \"Cluster 1\": [1, 4, 5],\n",
        "  \"Cluster 2\": [2, 3]\n",
        "}}\n",
        "\n",
        "Hierarchical:\n",
        "{{\n",
        "  \"Cluster 1\": {{\n",
        "    \"part a\": [1, 2, 16, 17],\n",
        "    \"part b\": [15, 18, 19]\n",
        "  }},\n",
        "  \"Cluster 2\": [3, 4, 5, 6],\n",
        "  \"Cluster 3\": {{\n",
        "    \"deep subcluster\": {{\n",
        "      \"type x\": [22, 39],\n",
        "      \"type y\": [40]\n",
        "    }}\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Only include citation numbers in each list. Do not include summaries or explanations.\n",
        "\n",
        "Strings:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = cluster_prompt | model | parser\n",
        "\n",
        "result = chain.invoke({\"text\": \"\\n\".join(string_list)})\n",
        "\n",
        "match = re.search(r\"\\{.*\\}\", result.strip(), re.DOTALL)\n",
        "clusters = json.loads(match.group()) if match else {}\n",
        "\n",
        "def print_clusters(clusters, indent=0):\n",
        "    for key, value in clusters.items():\n",
        "        prefix = \" \" * indent\n",
        "        if isinstance(value, dict):\n",
        "            print(f\"{prefix}{key}:\")\n",
        "            print_clusters(value, indent + 2)\n",
        "        else:\n",
        "            print(f\"{prefix}{key}: {value}\")\n",
        "\n",
        "print(\"Clusters:\")\n",
        "print_clusters(clusters)\n",
        "\n",
        "with open(\"results/clustered_citations_nested.json\", \"w\") as f:\n",
        "    json.dump(clusters, f, indent=2)\n"
      ],
      "metadata": {
        "id": "eXO1gLSQjHvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### clustering\n",
        "\n",
        "\n",
        "import json\n",
        "from sklearn.cluster import KMeans\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from collections import defaultdict\n",
        "\n",
        "with open(\"/home/skumar/Langchain/.venv/results/summaries.json\", \"r\") as f:\n",
        "    citation_data = json.load(f)\n",
        "\n",
        "citation_ids = list(citation_data.keys())\n",
        "citation_texts = list(citation_data.values())\n",
        "\n",
        "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text:latest\")  # use \"llama3\" if you want full LLM embedding\n",
        "embeddings = embedding_model.embed_documents(citation_texts)\n",
        "\n",
        "k = 6\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "clusters_raw = defaultdict(list)\n",
        "for idx, label in enumerate(labels):\n",
        "    clusters_raw[label].append((citation_ids[idx], citation_texts[idx]))\n",
        "\n",
        "cluster_summaries = []\n",
        "for i, entries in clusters_raw.items():\n",
        "    cluster_text = \"\\n\".join([f\"[{cid}] {text}\" for cid, text in entries])\n",
        "    cluster_summaries.append((f\"Cluster {i+1}\", cluster_text))\n",
        "\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "cluster_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are a helpful assistant. You are given a group of research summaries, each prefixed by a citation number like [22].\n",
        "\n",
        "Your task is to analyze this group and organize it into labeled subgroups based on theme, technique, or topic.\n",
        "Return the result as a JSON object with structure like:\n",
        "\n",
        "{{\n",
        "  \"Main Theme of This Cluster\": {{\n",
        "    \"Subgroup A\": [22, 23],\n",
        "    \"Subgroup B\": [24, 25]\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Only use citation numbers in the output. Do not include summaries or explanations.\n",
        "\n",
        "Cluster label: {label}\n",
        "Entries:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"label\", \"text\"]\n",
        ")\n",
        "\n",
        "final_clusters = {}\n",
        "\n",
        "for label, cluster_text in cluster_summaries:\n",
        "    try:\n",
        "        result = (cluster_prompt | model | parser).invoke({\"label\": label, \"text\": cluster_text})\n",
        "        parsed = json.loads(result[result.find(\"{\"):])\n",
        "        final_clusters[label] = parsed\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {label}: {e}\")\n",
        "\n",
        "with open(\"results/hybrid_llm_embedding_clusters.json\", \"w\") as f:\n",
        "    json.dump(final_clusters, f, indent=2)\n",
        "print(\"Hybrid clustering complete. Output saved to results/hybrid_llm_embedding_clusters.json\")\n"
      ],
      "metadata": {
        "id": "wTPVDBewjPfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install matplotlib\n",
        "import json\n",
        "from sklearn.cluster import KMeans\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from collections import defaultdict\n",
        "\n",
        "with open(\"/home/skumar/Langchain/.venv/results/summaries.json\", \"r\") as f:\n",
        "    citation_data = json.load(f)\n",
        "\n",
        "citation_ids = list(citation_data.keys())\n",
        "citation_texts = list(citation_data.values())\n",
        "\n",
        "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text:latest\")  # use \"llama3\" if you want full LLM embedding\n",
        "embeddings = embedding_model.embed_documents(citation_texts)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Range of k values to test\n",
        "k_range = range(2, 11)\n",
        "inertias = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(embeddings)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "# Optional: Plot the elbow curve to visualize\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, inertias, marker='o')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Inertia (sum of squared distances)')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.grid(True)\n",
        "plt.savefig(\"results/elbow_curve.png\")  # Save plot instead of showing it (since this is often run headless)\n",
        "plt.close()\n",
        "\n",
        "# Automatically find \"elbow point\" using slope change (simple heuristic)\n",
        "def find_elbow_point(inertias):\n",
        "    diffs = [inertias[i] - inertias[i+1] for i in range(len(inertias)-1)]\n",
        "    slopes = [diffs[i] - diffs[i+1] for i in range(len(diffs)-1)]\n",
        "    return slopes.index(max(slopes)) + 2  # +2 because index offset\n",
        "\n",
        "optimal_k = find_elbow_point(inertias)\n",
        "print(f\"Optimal k (by elbow method): {optimal_k}\")\n",
        "\n",
        "\n",
        "k = optimal_k\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "clusters_raw = defaultdict(list)\n",
        "for idx, label in enumerate(labels):\n",
        "    clusters_raw[label].append((citation_ids[idx], citation_texts[idx]))\n",
        "\n",
        "cluster_summaries = []\n",
        "for i, entries in clusters_raw.items():\n",
        "    cluster_text = \"\\n\".join([f\"[{cid}] {text}\" for cid, text in entries])\n",
        "    cluster_summaries.append((f\"Cluster {i+1}\", cluster_text))\n",
        "\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "cluster_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are a helpful assistant. You are given a group of research summaries, each prefixed by a citation number like [22].\n",
        "\n",
        "Your task is to analyze this group and organize it into labeled subgroups based on theme, technique, or topic.\n",
        "Return the result as a JSON object with structure like:\n",
        "\n",
        "{{\n",
        "  \"Main Theme of This Cluster\": {{\n",
        "    \"Subgroup A\": [22, 23],\n",
        "    \"Subgroup B\": [24, 25]\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Only use citation numbers in the output. Do not include summaries or explanations.\n",
        "\n",
        "Cluster label: {label}\n",
        "Entries:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"label\", \"text\"]\n",
        ")\n",
        "\n",
        "final_clusters = {}\n",
        "\n",
        "for label, cluster_text in cluster_summaries:\n",
        "    try:\n",
        "        result = (cluster_prompt | model | parser).invoke({\"label\": label, \"text\": cluster_text})\n",
        "        parsed = json.loads(result[result.find(\"{\"):])\n",
        "        final_clusters[label] = parsed\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {label}: {e}\")\n",
        "\n",
        "with open(\"results/hybrid_llm_embedding_clusters.json\", \"w\") as f:\n",
        "    json.dump(final_clusters, f, indent=2)\n",
        "print(\"Hybrid clustering complete. Output saved to results/hybrid_llm_embedding_clusters.json\")\n"
      ],
      "metadata": {
        "id": "tWsLQeejjSmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Visualization\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import plotly.graph_objects as go\n",
        "import os\n",
        "\n",
        "# Path to your cluster JSON file\n",
        "json_path = \"results/hybrid_llm_embedding_clusters.json\"\n",
        "\n",
        "# Ensure the file exists\n",
        "if not os.path.exists(json_path):\n",
        "    raise FileNotFoundError(\"The clustering JSON file does not exist.\")\n",
        "\n",
        "# Load the data\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Prepare the treemap hierarchy\n",
        "labels = [\"Root\"]\n",
        "parents = [\"\"]\n",
        "\n",
        "for cluster, cluster_content in data.items():\n",
        "    labels.append(cluster)\n",
        "    parents.append(\"Root\")\n",
        "\n",
        "    for theme, subgroups in cluster_content.items():\n",
        "        theme_label = f\"{cluster} - {theme}\"\n",
        "        labels.append(theme_label)\n",
        "        parents.append(cluster)\n",
        "\n",
        "        if isinstance(subgroups, dict):\n",
        "            for subgroup, citations in subgroups.items():\n",
        "                labels.append(subgroup)\n",
        "                parents.append(theme_label)\n",
        "                for citation in citations:\n",
        "                    citation_label = f\"Citation {citation}\"\n",
        "                    labels.append(citation_label)\n",
        "                    parents.append(subgroup)\n",
        "        elif isinstance(subgroups, list):\n",
        "            for citation in subgroups:\n",
        "                citation_label = f\"Citation {citation}\"\n",
        "                labels.append(citation_label)\n",
        "                parents.append(theme_label)\n",
        "\n",
        "# Create the treemap\n",
        "fig = go.Figure(go.Treemap(\n",
        "    labels=labels,\n",
        "    parents=parents,\n",
        "    marker=dict(colorscale=\"Blues\"),\n",
        "    root=dict(color=\"white\"),\n",
        "    branchvalues=\"total\"\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Citation Clustering Hierarchy\",\n",
        "    margin=dict(t=50, l=25, r=25, b=25),\n",
        ")\n",
        "\n",
        "# Save and display\n",
        "fig.write_html(\"results/citation_cluster_tree_horizontal.html\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "6Jq3N5l1jXak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Load the full citation cluster data\n",
        "with open(\"results/hybrid_llm_embedding_clusters.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Create the graph\n",
        "dot = Digraph(comment=\"Citation Tree Diagram\", format='png')\n",
        "dot.attr('graph', rankdir='TB', size='10')\n",
        "\n",
        "# Recursive function to add nodes and edges\n",
        "def add_nodes_recursive(parent_id, content, prefix):\n",
        "    if isinstance(content, list):  # Leaf: list of citation IDs\n",
        "        for cid in content:\n",
        "            cid_node = f\"{prefix}_cid_{cid}\"\n",
        "            dot.node(cid_node, f\"[{cid}]\", shape='ellipse', style='filled', color='lightblue')\n",
        "            dot.edge(parent_id, cid_node)\n",
        "    elif isinstance(content, dict):  # Internal node\n",
        "        for key, value in content.items():\n",
        "            child_id = f\"{prefix}_{key.replace(' ', '_').replace('>', '').replace(':', '')}\"\n",
        "            dot.node(child_id, key, shape='box', style='filled', color='orange')\n",
        "            dot.edge(parent_id, child_id)\n",
        "            add_nodes_recursive(child_id, value, prefix=child_id)\n",
        "\n",
        "# Start from clusters\n",
        "for cluster_name, themes in data.items():\n",
        "    cluster_id = f\"cluster_{cluster_name.replace(' ', '_')}\"\n",
        "    dot.node(cluster_id, cluster_name, shape='box', style='filled', color='red')\n",
        "    add_nodes_recursive(cluster_id, themes, prefix=cluster_id)\n",
        "\n",
        "# Render the diagram to a file\n",
        "output_path = \"/home/skumar/Langchain/.venv/results/hybrid_llm_embedding_clusters.json\"\n",
        "dot.render(output_path, format=\"png\")\n",
        "\n",
        "output_path + \".png\"  # Return path for viewing\n"
      ],
      "metadata": {
        "id": "iXN-VtVKjiQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbformat\n",
        "import json\n",
        "import plotly.express as px\n",
        "\n",
        "with open(\"results/hybrid_llm_embedding_clusters.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "labels = []\n",
        "parents = []\n",
        "\n",
        "for cluster, structure in data.items():\n",
        "    labels.append(cluster)\n",
        "    parents.append(\"\")  # root\n",
        "    theme = list(structure.keys())[0]\n",
        "    for subgroup, citations in structure[theme].items():\n",
        "        labels.append(subgroup)\n",
        "        parents.append(cluster)\n",
        "        for cid in citations:\n",
        "            labels.append(f\"Citation {cid}\")\n",
        "            parents.append(subgroup)\n",
        "\n",
        "fig = px.sunburst(\n",
        "    names=labels,\n",
        "    parents=parents,\n",
        "    title=\"Citation Clusters and Subgroups\",\n",
        ")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "veIsX46rjjTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### citation_tree\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"results/hybrid_llm_embedding_clusters.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def build_hierarchy(data):\n",
        "    children = []\n",
        "    for cluster, structure in data.items():\n",
        "        theme = list(structure.keys())[0]\n",
        "        cluster_node = {\"name\": cluster, \"children\": []}\n",
        "        for subgroup, citations in structure[theme].items():\n",
        "            subgroup_node = {\"name\": subgroup, \"children\": [{\"name\": f\"Citation {c}\"} for c in citations]}\n",
        "            cluster_node[\"children\"].append(subgroup_node)\n",
        "        children.append(cluster_node)\n",
        "    return {\"name\": \"Root\", \"children\": children}\n",
        "\n",
        "hierarchy = build_hierarchy(data)\n",
        "\n",
        "with open(\"results/d3_citation_tree.json\", \"w\") as f:\n",
        "    json.dump(hierarchy, f, indent=2)\n",
        "\n",
        "print(\"D3-compatible JSON saved to results/d3_citation_tree.json\")\n"
      ],
      "metadata": {
        "id": "S7_YlXWCjpFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Load cluster data\n",
        "json_path = Path(\"results/hybrid_llm_embedding_clusters.json\")\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Build nested tree for D3.js\n",
        "def build_d3_tree(data):\n",
        "    root = {\"name\": \"Root\", \"children\": []}\n",
        "    for cluster, content in data.items():\n",
        "        cluster_node = {\"name\": cluster, \"children\": []}\n",
        "        for theme, subgroups in content.items():\n",
        "            theme_node = {\"name\": theme, \"children\": []}\n",
        "            for subgroup, citations in subgroups.items():\n",
        "                subgroup_node = {\n",
        "                    \"name\": subgroup,\n",
        "                    \"children\": [{\"name\": f\"Citation {cid}\"} for cid in citations]\n",
        "                }\n",
        "                theme_node[\"children\"].append(subgroup_node)\n",
        "            cluster_node[\"children\"].append(theme_node)\n",
        "        root[\"children\"].append(cluster_node)\n",
        "    return root\n",
        "\n",
        "d3_data = build_d3_tree(data)\n",
        "\n",
        "# Save the result\n",
        "output_path = Path(\"results/d3_tree_data.json\")\n",
        "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(d3_data, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ D3.js data saved to:\", output_path)\n"
      ],
      "metadata": {
        "id": "dmVuebL2jvv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kN0VfzDzjy1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8yyj739kMWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Timeline\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def draw_basic_timeline(years_json_path, figSize=(15, 10)):\n",
        "    with open(years_json_path, 'r', encoding='utf-8') as f:\n",
        "        years_map = json.load(f)\n",
        "\n",
        "    citations_with_years = []\n",
        "\n",
        "    for num_str, year_str in years_map.items():\n",
        "        try:\n",
        "            year = int(year_str)\n",
        "            citations_with_years.append({\n",
        "                \"number\": num_str,\n",
        "                \"year\": year\n",
        "            })\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    citations_with_years.sort(key=lambda x: x['year'])\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=figSize)\n",
        "    min_year = min(c['year'] for c in citations_with_years) - 1\n",
        "    max_year = max(c['year'] for c in citations_with_years) + 1\n",
        "\n",
        "    ax.hlines(0, min_year, max_year, color='gray', linestyle='-', linewidth=1.5)\n",
        "\n",
        "    ax.set_xlabel(\"Year\", fontsize=12)\n",
        "    ax.set_xticks(range(min_year, max_year + 1))\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.set_xlim(min_year, max_year)\n",
        "    ax.yaxis.set_visible(False)\n",
        "    ax.spines[['left', 'right', 'top']].set_visible(False)\n",
        "    ax.spines['bottom'].set_linewidth(1.5)\n",
        "\n",
        "    y_offset_factor = 0.1\n",
        "    y_positions = {}\n",
        "\n",
        "    for citation in citations_with_years:\n",
        "        year = citation['year']\n",
        "        label = f\"#{citation['number']}\"\n",
        "        current_y_offset = y_positions.get(year, 0)\n",
        "\n",
        "        if year % 2 == 0:\n",
        "            y_pos = y_offset_factor * ((current_y_offset  + 0.5 // 2) * (1 if current_y_offset % 2 == 0 else -1))\n",
        "            y_positions[year] = current_y_offset + 1\n",
        "        else:\n",
        "            y_pos = y_offset_factor * ((current_y_offset - 0.5 // 2 + 1) * (1 if current_y_offset % 2 == 0 else -1))\n",
        "            y_positions[year] = current_y_offset + 1\n",
        "\n",
        "        ax.plot(year, 0, 'o', color='darkblue', markersize=6)\n",
        "        ax.plot([year, year], [0, y_pos], color='skyblue', linestyle='--', linewidth=0.8)\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            xy=(year, y_pos),\n",
        "            xytext=(year, y_pos + (0.02 if y_pos > 0 else -0.02)),\n",
        "            fontsize=9,\n",
        "            ha='center',\n",
        "            va='bottom' if y_pos > 0 else 'top',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
        "            arrowprops=dict(arrowstyle=\"-\", color='gray', linewidth=0.5)\n",
        "        )\n",
        "\n",
        "    plt.title(\"Citation Timeline (Using Only years.json)\", fontsize=14, pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    try:\n",
        "        output_path = Path(years_json_path).with_name(\"Basic_Citations_Timeline.jpeg\")\n",
        "        fig.savefig(output_path, format=\"jpeg\", bbox_inches=\"tight\", dpi=300)\n",
        "        print(f\"Timeline image saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save image: {e}\")\n",
        "\n",
        "# ========== MAIN ==========\n",
        "if __name__ == \"__main__\":\n",
        "    draw_basic_timeline(\"/home/skumar/Langchain/.venv/results/years.json\")\n"
      ],
      "metadata": {
        "id": "89plKjBMkMG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PVZA3W4BkvN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BBIkfopwkvGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TdC6S2yTku1-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}